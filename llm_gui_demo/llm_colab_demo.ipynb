{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481978f1-3c5f-49f2-a781-f73ae80f69e2",
   "metadata": {},
   "source": [
    "# LLM chat demo\n",
    "\n",
    "for this one, you need to get permission to use the Llama 3.2 1b model from Meta via huggingface. Do that, and then get a huggingface token, and you'll have to log in at the next cell, with that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a7c2a-717e-4e9a-9b2c-e924b52ae9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q streamlit\n",
    "!npm install localtunnel\n",
    "!pip install --upgrade huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3e0b3-6ff9-4918-a010-ab024e6b48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copypasta and create \n",
    "\n",
    "%%writefile app.py\n",
    "\n",
    "### BACKEND\n",
    "import os\n",
    "import copy\n",
    "\n",
    "#local demo imports and config\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer, TextIteratorStreamer\n",
    "\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "torch_dtype = torch.float16 if torch_device in [\"cuda\", \"mps\"] else torch.float32\n",
    "\n",
    "llama_model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "                                           #  quantization_config=quantization_config, \n",
    "                                           torch_dtype=torch_dtype, \n",
    "                                           device_map=torch_device)\n",
    "\n",
    "llama_tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "streamer = TextStreamer(llama_tokenizer)\n",
    "\n",
    "\n",
    "# def llama32_1b_streamchat(messages):\n",
    "#     inputs = llama_tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "#     inputs = torch.tensor(inputs).to(torch_device).unsqueeze(0)\n",
    "#     stream = llama_model.generate(inputs, streamer=streamer, max_new_tokens = 256)\n",
    "#     return stream\n",
    "\n",
    "llama32_1b_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama_model,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    # streamer = streamer,\n",
    ")\n",
    "\n",
    "def llama32_1b_chat(messages) -> str: \n",
    "    \"simplifies pipeline output to only return generated text\"\n",
    "    outputs = llama32_1b_pipe(\n",
    "        messages,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    return outputs[-1]['generated_text'][-1]['content']\n",
    "    \n",
    "\n",
    "### FRONTEND\n",
    "import streamlit as st\n",
    "import random\n",
    "import time\n",
    "from llm_demo import llama32_1b_chat\n",
    "\n",
    "\n",
    "# Streamed response emulator\n",
    "def response_generator():\n",
    "    response = random.choice(\n",
    "        [\n",
    "            \"Hello there! How can I assist you today?\",\n",
    "            \"Hi, human! Is there anything I can help you with?\",\n",
    "            \"Do you need help?\",\n",
    "        ]\n",
    "    )\n",
    "    for word in response.split():\n",
    "        yield word + \" \"\n",
    "        time.sleep(0.05)\n",
    "\n",
    "\n",
    "st.title(\"llm chat demo\")\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [{\"role\": \"system\", \"content\": \"You are a helpful chatbot who will assist the end user as best as possible.\"}, \n",
    "                                 {\"role\": \"assistant\", \"content\": \"Hi there, how can I help you today?\"}\n",
    "                                ]\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    if message[\"role\"] != \"system\": # don't display system messages\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "# Accept user input\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    # Display user message in chat message container\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Display assistant response in chat message container\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        generated_response = llama32_1b_chat(st.session_state.messages)\n",
    "        response = st.write(generated_response)\n",
    "        # making this a stream with write_stream isn't that simple weirdly\n",
    "    # Add assistant response to chat history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": generated_response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e440172-ed43-4d95-a2fe-16d7fe563cab",
   "metadata": {},
   "source": [
    "# HOW TO RUN THE GUI:\n",
    "Run this next cell, and then click the link generated. When it asks for a password, copy and paste the printed IP address and click \"enter\". IT should now work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49dd1ec-4184-42b4-956d-05eafddd72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RF-demos",
   "language": "python",
   "name": "rf-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
